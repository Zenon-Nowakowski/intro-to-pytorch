{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1da470",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "When we talk about classification problems, we talk about the problem of learning a function that maps input data into one of several class labels. A simple example of such a task would be an image classifier; we have as input an image and we wish to know if that image is a picture of a cat or a dog. In this case, \"cat\" and \"dog\" are the class labels. We can use a variety of methods to learn a function that can ingest the pixels of an image and produce a decision that maps to either of those classes.\n",
    "\n",
    "We will talk more about deep learning (deep neural networks) in a later lecture, so we'll focus on what's commonly referred to as \"traditional\" methods for machine learning, using sklearn (remember: installable via `pip install scikit-learn`\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "We'll keep the terminology simple here, but there are multiple way of referring to the different aspects of this problem. What we refer to as the input is translated into a \"feature matrix\". In deep learning, those features are learned automatically using backpropagation. For more traditional methods, we will have to either take into the input data natively, or compute features on those data. For example, the iris dataset has a number of features: sepalWidth, sepalLength, petalLength, petalWidth. An example of a computed feature might be the ratio of sepalWidth to sepalLength.\n",
    "\n",
    "What we've referred to as the \"label\" is also often referred to as the \"target\". We typically use an 'X' (note the uppercase) to refer to the feature matrix, and 'y' (not the lowercase) to refer to the target. X is usually a matrix, where the rows are the samples and the columns are the specific features, and y is an array, where each element refers to a specific data sample. We can refer to these as \"parallel arrays\" as the elements line up: eg. the 0th element in the feature matrix is the same sample as the 0th element of the target array. Therefore, there must be the same number of rows in the feature matrix as there are elements in the target array.\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{cc} \n",
    "0.5 & 0.75 & 0.25 & 0.99\\\\\n",
    "0.9 & 0.62 & 0.12 & 0.87\\\\\n",
    "0.8 & 0.55 & 0.33 & 0.43\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{cc} \n",
    "0 \\\\ \n",
    "0  \\\\\n",
    "1\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "In the example above, we have a 3x4 feature matrix (3 samples, 4 features) and a 3-element target array. The first two rows have a class label of '0', and the last row has a class label of '1'.\n",
    "\n",
    "If we're trying to distinguish between two classes (eg. cats vs. dogs) then we have a 'binary classification' problem. If we have more than two classes, we'll refer to it as a 'multi-class classification' problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1157c59",
   "metadata": {},
   "source": [
    "### Classification Process\n",
    "\n",
    "When building a classifier, we have a couple steps that we need to do. The first thing we need to do is gather our labeled data. This means building our feature matrix and our target array. Upon doing so, we'll then need to split our data into 'training' and 'testing' sets, usually something close to an 80/20 split. There are also methods that we will need to do, such as cross validation and/or creating a validation split. But that is beyond the scope of this class for now.\n",
    "\n",
    "We will then use the training set to train our model. Once our model is trained, we will test it by using the testing set. Our performance on the testing set can be measured via certain standard metrics such as accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbeb27",
   "metadata": {},
   "source": [
    "### Types of Classifiers\n",
    "\n",
    "There are a large number of different classifier types. A good overview from sklearn can be found here: https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html \n",
    "\n",
    "We will focus on just three: Naïve Bayes, Random Forest, and Support Vector Machines (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df8b81",
   "metadata": {},
   "source": [
    "### Naïve Bayes\n",
    "\n",
    "Naïve Bayes (NB) is a very simple classifier based on Bayes Theorem, which you may be familiar with through your statistics classes. \n",
    "\n",
    "The basic thought behind NB is \"class conditional independence\". This means that each feature in our feature matrix is independent. Which might not always be the case! But it makes the math easier. The equation behind NB is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "P(h|D) = \\frac{P(D|h)P(h)}{P(D)}\n",
    "\\end{equation}\n",
    "\n",
    "The prior probability of h, the hypthosis is the probability of the hypothesis being true, is given by:\n",
    "\\begin{equation}\n",
    "P(h)\n",
    "\\end{equation}\n",
    "\n",
    "The probability of the data, known as the prior probability, is given by:\n",
    "\\begin{equation}\n",
    "P(D)\n",
    "\\end{equation}\n",
    "\n",
    "The probability of the hypothesis given the data is known as the posterior probability, is given by:\n",
    "\\begin{equation}\n",
    "P(h|D)\n",
    "\\end{equation}\n",
    "\n",
    "And finally, the probability of the data given that the hypothesis was true, known as the posterior probability, is given by:\n",
    "\\begin{equation}\n",
    "P(D|h)\n",
    "\\end{equation}\n",
    "\n",
    "NB works by:\n",
    "1. Calculating the prior probability\n",
    "2. Finding the likelihood probability for each attribute of the class - remember class conditional independence?\n",
    "3. Apply Bayes Formula to calculate the posterior probability\n",
    "4. Find the highest class probability for a given input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcfa8c",
   "metadata": {},
   "source": [
    "### Preparing our data\n",
    "\n",
    "We'll take a look at our data, which we'll use for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_json('iris.json')\n",
    "species = iris['species'].unique()\n",
    "iris['species'].replace(species,list(range(0,len(species))), inplace=True)\n",
    "y = iris['species'].to_numpy()  # recall sometimes we have to reshape the array, but not for NB .reshape(-1,1)\n",
    "X = iris[['sepalLength','sepalWidth','petalLength','petalWidth']]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.15) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c9d8a",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "\n",
    "Now that we have our data, we'll build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98260d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e93de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to evaluate\n",
    "\n",
    "accuracy = accuracy_score(predicted, y_test)\n",
    "print('Classification accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78e1d0",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest (RF) is a pretty flexible model, as you can use RF for both regression and classification tasks. We will focus on using RF for classification tasks.\n",
    "\n",
    "The other unique feature of RF is that it is an ensemble method; the RF is built from a \"forest\" of decision trees, which work exactly as their name suggests. A decision tree takes in a subset of all the features from the feature matrix, as makes a classification decision. The RF is then a collection of those decision trees, each of which contribute a part to the whole, and their combined decisions are used to make the overall classification determination. \n",
    "\n",
    "We'll reuse our data and see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19702d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfbce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c519a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(predicted, y_test)\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73847d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb97d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can visualize what decision trees are doing within the RF\n",
    "print('Number of decision trees: {}'.format(len(rf.estimators_)))\n",
    "for i in range(1):\n",
    "    tree = rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree, feature_names=X_train.columns, class_names=species, filled=True, max_depth=10, impurity=False, proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71471b1",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "While Support Vector Machines (SVM) can be used to solve regression problems, they are most commonly applied to classification problems. They are an extremely powerful model, and prior to deep learning, was a very common and useful model used in classification tasks. SVMs can work on both continuous and categorical data.\n",
    "\n",
    "Recall our feature matrix. The size of our feature matrix - in our case, the iris data - has a dimensionality equal to the number of features. So our iris data has dimensionality equal to 4. That means that all of our data exists in a 4-dimensional space. This is hard to visualize obviously, so we'll take some short cuts when we get to that point.\n",
    "\n",
    "SVMs attempt to learn a hyperplane (recall a cartesian 2D plane) that separates our data in the n-dimensional space. The objective of the SVM algorithm is to find a maximum marginal hyperplane, meaning that it maximizes the distance between the classes in the n-dimensional space.\n",
    "\n",
    "One quick note: sometimes our data is not linearly separable. I'll draw an example on the board. If this is the case, some SVM models can use something called a \"kernel trick\" to embed the data into a higher dimensional space where the data is linearly separable. There are many different types of kernels: linear, polynomial, and radial basis function. RBFs map the input data into an infinite dimensional space. We won't go into the details. The performance of our kernel is usually determined by several hyperparameters, most notably \"gamma\", which ranges from 0 to 1 and indicates how much deference the model should pay to the training data. A value of 1 for gamma will result in the development of a hyperplane that perfectly separates the training data, which is called \"overfitting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31456f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = svm.SVC(kernel='linear')\n",
    "svc_model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f8e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(predictions, y_test)\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the dimensionality of the SVM, it's hard to visualize, so we'll use PCA to reduce it down to 2D so we\n",
    "# can look at it.\n",
    "# Code grabbed from: https://stackoverflow.com/questions/51297423/plot-scikit-learn-sklearn-svm-decision-boundary-surface\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "Xreduced = pca.fit_transform(X_train)\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "model = svm.SVC(kernel='linear')\n",
    "clf = model.fit(Xreduced, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# title for the plots\n",
    "title = ('Decision surface of linear SVC ')\n",
    "# Set-up grid for plotting.\n",
    "X0, X1 = Xreduced[:, 0], Xreduced[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_title('Decison surface using the PCA transformed/projected features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d3804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
