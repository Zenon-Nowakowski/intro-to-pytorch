{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4622e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13206af2",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "So far we have reviewed regression tasks (using Linear Regressin) and classification (using SVMs, Naïve Bayes, and Random Forests). Recall that regression involves predicting continuous values, and classification involves predicting a class label (discrete, categorical data). Today we will talk about clustering.\n",
    "\n",
    "Clustering is a form of unsupervised learning. Unsupervised means that we don't have a 'y' - a target / label vector that tells us what class a data item belongs to. For this, we may have a whole bunch of data that we don't know what's in there, and we just want to group the data into clusters by how similar they are.\n",
    "\n",
    "In order to do that, we have to learn a representation of those data in some n-dimensional space, and then cluster them together in that n-dimensional space. We can then view the results of that clustering by projecting those features down to a 2- or 3-dimensional space and plotting them using something like matplotlib.\n",
    "\n",
    "The most popular clustering algorithm is called 'k-means', and there are many variants of that family of algorithms. We'll keep it pretty simple to introduce the concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d2e0e",
   "metadata": {},
   "source": [
    "### k-means\n",
    "\n",
    "K-mean, like all clustering algorithms, attempts to learn some underlying representation of our data, and then group those data into clusters. The 'k' in 'k-means' refers to the number of clusters in our data. There are methods by which we can learn the optimal number of k, which overcomes ones main weakness of the algorithm.\n",
    "\n",
    "More specifically, k refers to the number of cluster centroids. Data items are 'clustered' based on which centroid they are closest to in that n-dimensional space. The objective of clustering algorithms such as k-means is to discover the optimal location of each cluster centroid such that the data can be partitioned into k-clusters such that the separability between the clusters is maximized. Note that the 'means' in k-means refers to the method by which we find the centroid.\n",
    "\n",
    "k-mean starts by randomly selecting k-centroids to act as the starting point. Then the algorithm iteratively performs calculations to further refine the centroid locations. The algorithm completes either when the maximum number of iterations have been performed, or when the centroids are stable enough - meaning that they don't change their values after a specified number of iterations and the data samples don't change their assigned clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41da14",
   "metadata": {},
   "source": [
    "### Mathematic Formation\n",
    "\n",
    "We first observe a set of random centroids: ${c_1, c_2,...,c_k}$. Then each iteration performs the following operations:\n",
    "\n",
    "1. Assignment\n",
    "\n",
    "$S_i = \\{x_p:||x_p - c_i||^2 \\leq ||x_p - c_j||^2 \\forall j, 1 \\leq j \\leq k\\}$\n",
    "\n",
    "In the assignment step, we assign each data sample $x_p$ to a set of data samples $S_i$ based on the cluster $c_i$ that it is closest to, based on the Euclidean distance.\n",
    "\n",
    "2. Update\n",
    "\n",
    "$c_i = \\frac{1}{|S_{k}|}\\sum_{x_j \\in S_i} x_j$\n",
    "\n",
    "In the update steps, we recalculate the cluster centroids by taking the average position of the elements in each set associated with a cluster.\n",
    "\n",
    "Note that the distance between elements here is a fairly naïve Euclidean distance; however, there is a whole field of mathematics devoted to finding the distances between objects - Measure Theory - and we can get quite esoteric with how we choose to group things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbcfe53",
   "metadata": {},
   "source": [
    "### Looking at the Data\n",
    "\n",
    "The first thing that we're going to do is look at the iris data and see how separable the features are. Note that we could do this for the regression and classification tasks as well. We'll do this for one of our features, but could do them for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b273315",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_json('iris.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa = iris.loc[iris.species == 'setosa']\n",
    "versicolor = iris.loc[iris.species == 'versicolor']\n",
    "virginica = iris.loc[iris.species == 'virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(color='r', xlim=(0,8), ylim=(0,5))\n",
    "setosa.plot(kind='scatter', x='sepalLength', y='sepalWidth', **kwargs)\n",
    "kwargs = dict(color='g', xlim=(0,8), ylim=(0,5))\n",
    "versicolor.plot(kind='scatter', x='sepalLength', y='sepalWidth', **kwargs)\n",
    "kwargs = dict(color='b', xlim=(0,8), ylim=(0,5))\n",
    "virginica.plot(kind='scatter', x='sepalLength', y='sepalWidth', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babf225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now onto clustering. We'll set the number of clusters to be the number of classes\n",
    "\n",
    "k = len(iris.species.unique())\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "X = iris[['sepalWidth','sepalLength','petalWidth','petalWidth']].values\n",
    "y = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9537537",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = iris.columns \n",
    "plt.scatter(X[y == 0, 0], \n",
    "            X[y == 0, 1], \n",
    "            c='red', \n",
    "            label='setosa') \n",
    "plt.scatter(X[y == 1, 0], \n",
    "            X[y == 1, 1], \n",
    "            c='green', \n",
    "            label='versicolour') \n",
    "plt.scatter(X[y == 2, 0], \n",
    "            X[y == 2, 1], \n",
    "            c='blue', \n",
    "            label='virginica') \n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='black', s=100, label='Centroids')\n",
    "  \n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc7616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
